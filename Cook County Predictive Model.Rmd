---
title: "BIG DATA ANALYTICS FINAL PROJECT"
subtitle: "PROPERTY ASSESSMENT CHALLENGE FOR COOK COUNTY ILLINOIS"
author: Yuqing Du, Tran (Kaitlyn) Pham
output:
  html_document:
  theme: simplex
  fig_caption: true
---

# 1.Data Overview
## 1.1 Load Packages and Import Data

```{r}
options(scipen = 999)
library("tidyverse")
library("dplyr")
library("ggplot2")

historic_raw <- read_csv("historic_property_data.csv", show_col_types = FALSE)
codebook <- read_csv("codebook.csv", show_col_types = FALSE)

dim(historic_raw)

```

## 1.2 Select Predictors from Codebook
```{r}
# Identify predictor variables
predictors <- codebook %>%
  filter(var_is_predictor == TRUE) %>%
  pull(var_name_standard)

# Match predictors with columns in datasets
predictor_vars1 <- predictors[predictors %in% colnames(historic_raw)]

# Filter relevant columns for the historic dataset
historic <- historic_raw %>%
  select(sale_price, all_of(predictor_vars1))

historic <- historic %>%
  select_if(~ !is.character(.))

historic <- historic %>%
  mutate(across(where(is.logical), ~ as.integer(.)))

# Check new dataset dimensions
dim(historic)

```

## 1.3 Explore Dataset Structure

```{r}
# View the structure of the datasets
head(historic)
names(historic)
str(historic)
```


## 1.4 Handle Missing Values

```{r}
# Count and summarize missing values
sum(is.na(historic)) 
colSums(is.na(historic))

# Remove columns with >50% missing values
high_na_columns_h <- colSums(is.na(historic)) / nrow(historic) > 0.5
historic1 <- historic[, !high_na_columns_h]
ncol(historic1)

# Replace missing values for numeric columns with the median
numeric_columns_h <- sapply(historic1, is.numeric)
historic1[numeric_columns_h] <- 
  lapply(historic1[numeric_columns_h], 
         function(x) 
           ifelse(is.na(x), median(x, na.rm = TRUE), x)
         )

# Replace missing values for categorical columns with the mode
categorical_columns_h <- sapply(historic1, is.factor)
historic1[categorical_columns_h] <- 
  lapply(historic1[categorical_columns_h], 
         function(x) 
           ifelse(is.na(x), names(which.max(table(x))), x)
         )

# Replace logical column missing values with mode
mode_ind_garage <- names(which.max(table(historic1[["ind_garage"]], useNA = "no")))
historic1[["ind_garage"]][is.na(historic1[["ind_garage"]])] <- mode_ind_garage

# Confirm no missing values remain
sum(is.na(historic1)) 

```

## 1.5 Exploratory Data Analysis (EDA)

```{r}
# Descriptive statistics
summary(historic1)

# Density map of sales prices
ggplot(historic1, aes(x = sale_price)) + 
  geom_histogram(bins = 30, 
                 fill = "pink",
                 color = "darkgrey") +
  labs(title = "Distribution of Sale Price", x = "Sale Price", y = "Frequency")
```

## 1.6 Correlation Analysis and Feature Selection

```{r}
# Select numeric variables and calculate correlation matrix
numeric_data_h <- select(historic1, where(is.numeric))
cor_matrix_h <- cor(numeric_data_h, use = "complete.obs")

# Identify and remove highly correlated variables (threshold > 0.8)
high_cor_h <- which(abs(cor_matrix_h) > 0.8, arr.ind = TRUE)
high_cor_h <- high_cor_h[high_cor_h[, 1] < high_cor_h[, 2], ] 
exclude_vars_h <- unique(as.vector(high_cor_h[, 2]))
colnames(cor_matrix_h)[exclude_vars_h]

# Create cleaned dataset
historic_cleaned <- historic1[, -exclude_vars_h]
ncol(historic_cleaned)
```



# 2. Creat the Model

```{r}
library("glmnet")
library("boot")
```

```{r}
library("caret")
library("randomForest")
```

## 2.1 Split Data into Training and Testing Sets
```{r}
set.seed(133)  
split <- createDataPartition(historic_cleaned$sale_price, p = 0.8, list = TRUE)
train_data <- historic_cleaned[split$Resample1, ]
test_data <- historic_cleaned[-split$Resample1, ]
```

## 2.2 Feature Selection by Lasso
```{r}
# Prepare data for Lasso
x_train <- model.matrix(sale_price ~ . - 1, data = train_data)  
y_train <- train_data$sale_price

# Train Lasso model and select optimal lambda
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, type.measure = "mse", nfolds = 10)
lasso_model

opt_lambda <- lasso_model$lambda.min
opt_lambda

```

### Extract significant features
```{r}

coef_selected <- coef(lasso_model, s = opt_lambda)

vars <- rownames(coef_selected)[coef_selected[, 1] != 0 & rownames(coef_selected) != "(Intercept)"]

significant_vars <- vars[vars %in% names(historic_cleaned)]

print(significant_vars)
length(significant_vars)

```

### Create Dataset with Selected Features
```{r}
historic_cleaned_afterLASSO <- historic_cleaned[, c(significant_vars, "sale_price")]
train_data_new <- historic_cleaned_afterLASSO[split$Resample1, ]
test_data_new <- historic_cleaned_afterLASSO[-split$Resample1, ]

dim(train_data_new)
dim(test_data_new)
```

# 2.3 Build Neural Networks model based on the new dataset
```{r}
library(nnet)


# Scale data for Neural Networks
scaled_train <- as.data.frame(scale(train_data_new))
scaled_test <- as.data.frame(scale(test_data_new))

# Train Neural Network
nn_model <- nnet(sale_price ~ ., data = scaled_train, size = 10, linout = TRUE, maxit = 200)
```

```{r}
# Test and evaluate the model
nn_test <- predict(nn_model, newdata = scaled_test)
nn_mse <- mean((scaled_test$sale_price - nn_test)^2)
print(paste("Neural Network Test MSE:", nn_mse))

```
# 3 Use the neural network model to predict sale_prices in the predict data

## 3.1 Prepare the data
```{r}
#load the data
predict_raw <- read_csv("predict_property_data.csv", show_col_types = FALSE)

# Make sure the predict data contains all the variables used by the final model
predict <- predict_raw %>%
  select(all_of(significant_vars))

dim(predict)

# Replace the missing value
predict_clean <- predict %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
  mutate(across(where(is.factor), ~ifelse(is.na(.), names(which.max(table(.))), .)))

sum(is.na(predict_clean)) 

```
# 3.2 Predict
```{r}

# Scale the data
mean_price <- mean(train_data$sale_price, na.rm = TRUE)
sd_price <- sd(train_data$sale_price, na.rm = TRUE)

scale_function <- function(x) {
  x_mean <- mean(x, na.rm = TRUE)
  x_sd <- sd(x, na.rm = TRUE)
  if (x_sd == 0) {
    return(rep(0, length(x)))  # if sd==0, return 0
  } else {
    return((x - x_mean) / x_sd)
  }
}


scaled_predict <- as.data.frame(lapply(predict_clean, scale_function))

# Predict and inverse standardization
nn_predictions <- predict(nn_model, newdata = scaled_predict)
assessed_values <- nn_predictions * sd_price + mean_price

# Output
results_df <- data.frame(pid = predict_raw$pid, assessed_value = assessed_values)
results_df$assessed_value <- ifelse(is.na(results_df$assessed_value) | results_df$assessed_value < 0, 0, results_df$assessed_value)


dim(results_df)
head(results_df)


```

```{r}
# Output to a csv file
write.csv(results_df, "D:\\UIUC\\FIN550-R\\final project\\assessed_value.csv", row.names = FALSE)
```



